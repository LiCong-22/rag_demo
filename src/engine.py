# src/engine.py
import torch
import time
import numpy as np
from langchain_milvus import Milvus
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from rank_bm25 import BM25Okapi
import jieba
from src.config import (
    MILVUS_URI, COLLECTION_NAME,
    LLM_MODEL_PATH, EMBEDDING_MODEL_PATH,
    LLM_TYPE, OPENAI_API_KEY, OPENAI_MODEL,
    ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_BASE_URL,
    ENABLE_HYDE, ENABLE_QUERY_EXPANSION,
    EXPANSION_COUNT, RETRIEVAL_K
)

# ==================== åŠ¨æ€é…ç½® ====================
# è¿™äº›å˜é‡å¯ä»¥åœ¨è¿è¡Œæ—¶ä¿®æ”¹
_enable_hyde = ENABLE_HYDE
_enable_query_expansion = ENABLE_QUERY_EXPANSION

def get_rag_config():
    """è·å–å½“å‰ RAG é…ç½®"""
    return {
        "enable_hyde": _enable_hyde,
        "enable_query_expansion": _enable_query_expansion,
        "expansion_count": EXPANSION_COUNT,
        "retrieval_k": RETRIEVAL_K
    }

def set_rag_config(enable_hyde: bool = None, enable_query_expansion: bool = None):
    """åŠ¨æ€ä¿®æ”¹ RAG é…ç½®"""
    global _enable_hyde, _enable_query_expansion
    if enable_hyde is not None:
        _enable_hyde = enable_hyde
    if enable_query_expansion is not None:
        _enable_query_expansion = enable_query_expansion
    return get_rag_config()

class RAGEngine:
    def __init__(self):
        print(">>> [1/4] åˆå§‹åŒ– Embedding æ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_PATH,
            model_kwargs={'device': 'cuda'},
            encode_kwargs={'normalize_embeddings': True}
        )

        print(">>> [2/4] è¿æ¥ Milvus...")
        self.vector_store = Milvus(
            embedding_function=self.embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
        )

        print(">>> [3/4] åˆå§‹åŒ– BM25 æ£€ç´¢å™¨...")
        self._init_bm25()

        print(f">>> [4/4] åŠ è½½ LLM æ¨¡å‹ ({LLM_TYPE})...")

        # æ ¹æ® LLM_TYPE é€‰æ‹©ä¸åŒçš„åˆå§‹åŒ–æ–¹å¼
        if LLM_TYPE == "local":
            self.llm = self._init_local_llm()
        elif LLM_TYPE == "openai":
            self.llm = self._init_openai_llm()
        elif LLM_TYPE == "anthropic":
            self.llm = self._init_anthropic_llm()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM_TYPE: {LLM_TYPE}")
        
        # æ„å»º RAG é“¾
        template = """ä½ æ˜¯ä¸€ä¸ªæ±½è½¦ç”µå­è½¯ä»¶ç ”å‘åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

        è¦æ±‚ï¼š
        1. åªå›ç­”ä¸€æ¬¡ï¼Œä¸è¦é‡å¤ç›¸åŒå†…å®¹
        2. å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸­ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´"çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        3. å›ç­”ç®€æ´ä¸“ä¸šï¼Œä¸è¦ç¼–é€ 

        å·²çŸ¥ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š
        {question}

        å›ç­”ï¼š
        """
        prompt = PromptTemplate.from_template(template)
        
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        self.rag_chain = (
            {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        self.retriever_with_sources = self.vector_store.as_retriever(search_kwargs={"k": RETRIEVAL_K})

        print(">>> âœ… RAG å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")

    def _init_bm25(self):
        """åˆå§‹åŒ– BM25 æ£€ç´¢å™¨"""
        # ä» Milvus è·å–æ‰€æœ‰æ–‡æ¡£ç”¨äºæ„å»º BM25 ç´¢å¼•
        all_docs = self.vector_store.similarity_search("", k=10000)
        self.bm25_corpus = [doc.page_content for doc in all_docs]
        self.bm25_docs = all_docs  # ä¿ç•™åŸå§‹æ–‡æ¡£å¼•ç”¨

        # ä¸­æ–‡åˆ†è¯
        tokenized_corpus = [list(jieba.cut(doc)) for doc in self.bm25_corpus]
        self.bm25 = BM25Okapi(tokenized_corpus)
        print(f"    BM25 ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.bm25_corpus)} ä¸ªæ–‡æ¡£")

    def _bm25_search(self, query: str, k: int = 5):
        """BM25 å…³é”®è¯æ£€ç´¢"""
        tokenized_query = list(jieba.cut(query))
        scores = self.bm25.get_scores(tokenized_query)
        top_indices = np.argsort(scores)[::-1][:k]
        return [self.bm25_docs[i] for i in top_indices if scores[i] > 0]

    def _rrf_fusion(self, results_list: list, k: int = 60):
        """RRF (Reciprocal Rank Fusion) æ··åˆæ£€ç´¢ç®—æ³•"""
        doc_scores = {}

        for results in results_list:
            for rank, doc in enumerate(results):
                doc_key = doc.page_content
                if doc_key not in doc_scores:
                    doc_scores[doc_key] = {"doc": doc, "score": 0}
                doc_scores[doc_key]["score"] += 1.0 / (k + rank + 1)

        # æŒ‰åˆ†æ•°æ’åº
        sorted_docs = sorted(doc_scores.values(), key=lambda x: x["score"], reverse=True)
        return [item["doc"] for item in sorted_docs]

    def _extract_text(self, result) -> str:
        """ä» LLM è¿”å›ç»“æœä¸­æå–æ–‡æœ¬"""
        if hasattr(result, 'content'):
            content = result.content
            if isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        if item.get('type') == 'text':
                            return item.get('text', '')
            elif isinstance(content, str):
                return content
        return str(result)

    def _generate_hypothetical_doc(self, question: str) -> str:
        """ç”Ÿæˆå‡è®¾æ–‡æ¡£ (HyDE)"""
        prompt = f"""è¯·æ ¹æ®é—®é¢˜ç”Ÿæˆä¸€ä¸ªå¯èƒ½åŒ…å«ç­”æ¡ˆçš„å‡è®¾æ–‡æ¡£ç‰‡æ®µã€‚
è¦æ±‚ï¼šç›´æ¥ç»™å‡ºå‡è®¾æ–‡æ¡£å†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€è§£é‡Šã€‚é—®é¢˜è¶Šç®€æ´è¶Šå¥½ã€‚

é—®é¢˜ï¼š{question}
å‡è®¾æ–‡æ¡£ï¼š"""
        try:
            result = self.llm.invoke(prompt)
            return self._extract_text(result)
        except Exception as e:
            print(f"    âš ï¸ HyDE ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def _expand_query(self, question: str, num_expansions: int = None) -> list[str]:
        """ç”ŸæˆåŒä¹‰æŸ¥è¯¢ (æŸ¥è¯¢æ‰©å±•)"""
        if num_expansions is None:
            num_expansions = EXPANSION_COUNT

        prompt = f"""ç”Ÿæˆ {num_expansions} ä¸ªä¸ä»¥ä¸‹é—®é¢˜æ„æ€ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„é—®æ³•ã€‚
è¦æ±‚ï¼š
1. æ¯è¡Œä¸€ä¸ªé—®æ³•ï¼Œä¸è¦æœ‰ç¼–å·æˆ–å‰ç¼€
2. ç›´æ¥è¿”å›é—®æ³•åˆ—è¡¨ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Š

åŸå§‹é—®é¢˜ï¼š{question}
åŒä¹‰é—®æ³•ï¼š"""
        try:
            result = self.llm.invoke(prompt)
            expanded = self._extract_text(result).strip().split('\n')
            # è¿‡æ»¤ç©ºè¡Œå¹¶è¿”å›
            expanded = [q.strip() for q in expanded if q.strip()]
            return [question] + expanded[:num_expansions]
        except Exception as e:
            print(f"    âš ï¸ æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [question]

    def _hyde_search(self, question: str, k: int) -> list:
        """ä½¿ç”¨ HyDE è¿›è¡Œæ£€ç´¢"""
        hypothetical_doc = self._generate_hypothetical_doc(question)
        if hypothetical_doc:
            print(f"    ğŸ“ HyDE å‡è®¾æ–‡æ¡£: {hypothetical_doc[:100]}...")
            return self.vector_store.similarity_search(hypothetical_doc, k=k)
        return []

    def _expanded_search(self, question: str, k: int) -> list:
        """ä½¿ç”¨æŸ¥è¯¢æ‰©å±•è¿›è¡Œæ£€ç´¢"""
        expanded_queries = self._expand_query(question)
        if len(expanded_queries) > 1:
            print(f"    ğŸ” æ‰©å±•æŸ¥è¯¢: {expanded_queries}")

        all_results = []
        for query in expanded_queries:
            results = self.vector_store.similarity_search(query, k=k)
            all_results.append(results)

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        merged = []
        seen = set()
        for results in all_results:
            for doc in results:
                key = doc.page_content
                if key not in seen:
                    seen.add(key)
                    merged.append(doc)
        return merged

    def hybrid_search(self, query: str, k: int = None):
        """æ··åˆæ£€ç´¢ï¼šå‘é‡ + BM25 + HyDE + æŸ¥è¯¢æ‰©å±•"""
        if k is None:
            k = RETRIEVAL_K

        print(f">>> å¼€å§‹æ£€ç´¢ (HyDE={_enable_hyde}, æ‰©å±•={_enable_query_expansion})")

        all_results = []

        # 1. åŸºç¡€å‘é‡æ£€ç´¢
        vector_results = self.vector_store.similarity_search(query, k=k)
        all_results.append(vector_results)

        # 2. BM25 æ£€ç´¢
        bm25_results = self._bm25_search(query, k=k)
        all_results.append(bm25_results)

        # 3. HyDE æ£€ç´¢ (å¦‚æœå¯ç”¨)
        if _enable_hyde:
            hyde_results = self._hyde_search(query, k)
            if hyde_results:
                all_results.append(hyde_results)

        # 4. æŸ¥è¯¢æ‰©å±•æ£€ç´¢ (å¦‚æœå¯ç”¨)
        if _enable_query_expansion:
            expanded_results = self._expanded_search(query, k)
            if expanded_results:
                all_results.append(expanded_results)

        # RRF èåˆæ‰€æœ‰ç»“æœ
        fused_results = self._rrf_fusion(all_results, k=60)

        return fused_results[:k]

    def _init_local_llm(self):
        """åˆå§‹åŒ–æœ¬åœ° HuggingFace æ¨¡å‹"""
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )

        tokenizer = AutoTokenizer.from_pretrained(
            LLM_MODEL_PATH,
            trust_remote_code=True
        )

        model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_PATH,
            trust_remote_code=True,
            quantization_config=quantization_config,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
        )

        print(f"    æ¨¡å‹è®¾å¤‡ï¼š{next(model.parameters()).device}")

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            return_full_text=False
        )

        return HuggingFacePipeline(pipeline=pipe)

    def _init_openai_llm(self):
        """åˆå§‹åŒ– OpenAI API æ¨¡å‹"""
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=OPENAI_MODEL,
            temperature=0.1,
            api_key=OPENAI_API_KEY,
        )

    def _init_anthropic_llm(self):
        """åˆå§‹åŒ– Anthropic Claude æ¨¡å‹ (æ”¯æŒ MiniMax ç­‰å…¼å®¹ API)"""
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(
            model=ANTHROPIC_MODEL,
            temperature=0.1,
            anthropic_api_key=ANTHROPIC_API_KEY,
            base_url=ANTHROPIC_BASE_URL,
        )

    def query(self, question: str):
        start_total = time.time()
        print(f">>> æ­£åœ¨å¤„ç†ï¼š{question}")

        # è®¡æ—¶ï¼šæ··åˆæ£€ç´¢
        start_retrieval = time.time()
        docs = self.hybrid_search(question)
        retrieval_time = time.time() - start_retrieval

        # æ„å»º context
        context = "\n\n".join(doc.page_content for doc in docs)

        # è®¡æ—¶ï¼šLLM ç”Ÿæˆ
        start_llm = time.time()
        answer = self.llm.invoke(f"å·²çŸ¥ä¿¡æ¯ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š")
        llm_time = time.time() - start_llm

        total_time = time.time() - start_total

        print(f">>> æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}s (æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£)")
        print(f">>> LLM ç”Ÿæˆè€—æ—¶: {llm_time:.2f}s")
        print(f">>> æ€»è€—æ—¶: {total_time:.2f}s")

        # å¤„ç† LLM è¿”å›å€¼
        answer_text = ""
        thinking_text = ""

        try:
            if hasattr(answer, 'content'):
                content = answer.content
                # å¤„ç† MiniMax/Claude çš„æ€è€ƒæ¨¡å‹è¿”å›æ ¼å¼ (list with type field)
                if isinstance(content, list) and len(content) > 0:
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get('type', '')
                            if item_type == 'text':
                                answer_text = item.get('text', '')
                            elif item_type == 'thinking':
                                thinking_text = item.get('thinking', '')
                            elif item_type == 'output':
                                # æœ‰æ—¶æ˜¯ output å­—æ®µ
                                answer_text = item.get('text', item.get('output', str(item)))
                            else:
                                # æ™®é€šæ¨¡å‹å¯èƒ½è¿”å›å…¶ä»–æ ¼å¼ï¼Œå°è¯•ç›´æ¥æå–
                                answer_text = str(item)
                        else:
                            answer_text = str(item)
                elif isinstance(content, str):
                    # æ™®é€šå­—ç¬¦ä¸²æ ¼å¼ (å¦‚æœ¬åœ°æ¨¡å‹)
                    answer_text = content
                elif content is not None:
                    # å…¶ä»–æ ¼å¼
                    answer_text = str(content)
            elif isinstance(answer, str):
                # ç›´æ¥æ˜¯å­—ç¬¦ä¸²
                answer_text = answer
            else:
                answer_text = str(answer)
        except Exception as e:
            print(f"âš ï¸ è§£æå›ç­”æ—¶å‡ºé”™: {e}")
            answer_text = str(answer)

        return {
            "answer": answer_text,
            "thinking": thinking_text,
            "sources": [doc.page_content for doc in docs]
        }

engine = RAGEngine()